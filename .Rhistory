precision_hyp <- TP / (TP + FP)
recall_hyp    <- TP / (TP + FN)
f1_score_hyp  <- 2 * precision_hyp * recall_hyp / (precision_hyp + recall_hyp)
# Output results
cat("Classification Metrics for Predicting prevalentHypertension (All Features):\n")
cat("----------------------------------------------------------------------------\n")
cat(sprintf("Accuracy : %.4f\n", accuracy_hyp))
cat(sprintf("Precision: %.4f\n", precision_hyp))
cat(sprintf("Recall   : %.4f\n", recall_hyp))
cat(sprintf("F1 Score : %.4f\n", f1_score_hyp))
cat("\nConfusion Matrix:\n")
print(conf_mat_hyp)
# Logistic regression formula (you can expand this if needed)
chd_formula <- TenYearCHD ~ age + BMI + glucose + heartRate + currentSmoker + diabetes + prevalentHyp
# Fit logistic regression on scaled data
chd_model <- glm(chd_formula, data = framingham_scaled, family = "binomial")
# Predict probabilities and classify
framingham_scaled$chd_prob <- predict(chd_model, type = "response")
framingham_scaled$chd_pred <- ifelse(framingham_scaled$chd_prob > 0.5, 1, 0)
# Actual and predicted
actual <- framingham_scaled$TenYearCHD
predicted <- framingham_scaled$chd_pred
# Confusion matrix
conf_mat <- table(Predicted = predicted, Actual = actual)
TP <- conf_mat["1", "1"]
TN <- conf_mat["0", "0"]
FP <- conf_mat["1", "0"]
FN <- conf_mat["0", "1"]
# Metrics
accuracy  <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1_score  <- 2 * precision * recall / (precision + recall)
# Print metrics
cat("Classification Metrics for Predicting TenYearCHD:\n")
cat("-------------------------------------------------\n")
cat(sprintf("Accuracy : %.4f\n", accuracy))
cat(sprintf("Precision: %.4f\n", precision))
cat(sprintf("Recall   : %.4f\n", recall))
cat(sprintf("F1 Score : %.4f\n", f1_score))
cat("\nConfusion Matrix:\n")
print(conf_mat)
# Explicitly define all predictors (excluding the response and any derived variables)
predictor_vars <- c("male", "age", "education", "currentSmoker", "cigsPerDay",
"BPMeds", "prevalentStroke", "diabetes", "totChol", "sysBP",
"diaBP", "BMI", "heartRate", "glucose", "TenYearCHD")
# Subset the data
predictors <- framingham_scaled[, predictor_vars]
# Fit logistic regression model
hyp_model <- glm(framingham_scaled$prevalentHyp ~ ., data = predictors, family = "binomial")
# Predict probabilities and labels
hyp_prob <- predict(hyp_model, type = "response")
hyp_pred <- ifelse(hyp_prob > 0.5, 1, 0)
# Actual vs predicted
actual_hyp <- framingham_scaled$prevalentHyp
# Confusion matrix
conf_mat_hyp <- table(Predicted = hyp_pred, Actual = actual_hyp)
TP <- conf_mat_hyp["1", "1"]
TN <- conf_mat_hyp["0", "0"]
FP <- conf_mat_hyp["1", "0"]
FN <- conf_mat_hyp["0", "1"]
# Metrics
accuracy_hyp  <- (TP + TN) / (TP + TN + FP + FN)
precision_hyp <- TP / (TP + FP)
recall_hyp    <- TP / (TP + FN)
f1_score_hyp  <- 2 * precision_hyp * recall_hyp / (precision_hyp + recall_hyp)
# Output results
cat("Classification Metrics for Predicting prevalentHypertension (All Features):\n")
cat("----------------------------------------------------------------------------\n")
cat(sprintf("Accuracy : %.4f\n", accuracy_hyp))
cat(sprintf("Precision: %.4f\n", precision_hyp))
cat(sprintf("Recall   : %.4f\n", recall_hyp))
cat(sprintf("F1 Score : %.4f\n", f1_score_hyp))
cat("\nConfusion Matrix:\n")
print(conf_mat_hyp)
# -----------------------------
# Feature Importance (absolute standardized coefficients)
# -----------------------------
coefs <- summary(hyp_model)$coefficients
feature_importance <- abs(coefs[-1, "Estimate"])  # exclude intercept
importance_df <- data.frame(Feature = names(feature_importance),
Importance = feature_importance)
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]
cat("\nFeature Importance (by standardized coefficient magnitude):\n")
print(importance_df)
# Optional: plot
library(ggplot2)
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "darkred") +
coord_flip() +
labs(title = "Feature Importance for Predicting Hypertension",
x = "Feature", y = "Coefficient Magnitude (Standardized)") +
theme_minimal()
# Load necessary package
library(ggplot2)
# Define all original predictor variables
predictor_vars <- c("male", "age", "education", "currentSmoker", "cigsPerDay",
"BPMeds", "prevalentStroke", "prevalentHyp", "diabetes",
"totChol", "sysBP", "diaBP", "BMI", "heartRate", "glucose")
# Subset predictors and fit the model
predictors <- framingham_scaled[, predictor_vars]
chd_model <- glm(framingham_scaled$TenYearCHD ~ ., data = predictors, family = "binomial")
# Predict probabilities and binary labels
chd_prob <- predict(chd_model, type = "response")
chd_pred <- ifelse(chd_prob > 0.5, 1, 0)
# Actual vs predicted
actual <- framingham_scaled$TenYearCHD
predicted <- chd_pred
# Confusion matrix
conf_mat <- table(Predicted = predicted, Actual = actual)
TP <- conf_mat["1", "1"]
TN <- conf_mat["0", "0"]
FP <- conf_mat["1", "0"]
FN <- conf_mat["0", "1"]
# Classification metrics
accuracy  <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1_score  <- 2 * precision * recall / (precision + recall)
# Print metrics
cat("Classification Metrics for Predicting TenYearCHD (All Features):\n")
cat("-----------------------------------------------------------------\n")
cat(sprintf("Accuracy : %.4f\n", accuracy))
cat(sprintf("Precision: %.4f\n", precision))
cat(sprintf("Recall   : %.4f\n", recall))
cat(sprintf("F1 Score : %.4f\n", f1_score))
cat("\nConfusion Matrix:\n")
print(conf_mat)
# -----------------------------
# Feature Importance (standardized coefficients)
# -----------------------------
coefs <- summary(chd_model)$coefficients
feature_importance <- abs(coefs[-1, "Estimate"])  # remove intercept
importance_df <- data.frame(Feature = names(feature_importance),
Importance = feature_importance)
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]
cat("\nFeature Importance (by standardized coefficient magnitude):\n")
print(importance_df)
# Plot importance
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +
labs(title = "Feature Importance for Predicting TenYearCHD",
x = "Feature", y = "Coefficient Magnitude (Standardized)") +
theme_minimal()
# -----------------------------
# Feature Importance (absolute standardized coefficients)
# -----------------------------
coefs <- summary(hyp_model)$coefficients
feature_importance <- abs(coefs[-1, "Estimate"])  # exclude intercept
importance_df <- data.frame(Feature = names(feature_importance),
Importance = feature_importance)
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]
cat("\nFeature Importance (by standardized coefficient magnitude):\n")
print(importance_df)
# Optional: plot
library(ggplot2)
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "darkred") +
coord_flip() +
labs(title = "Feature Importance for Predicting Hypertension",
x = "Feature", y = "Coefficient Magnitude (Standardized)") +
theme_minimal()
# Explicitly define all predictors (excluding the response and any derived variables)
predictor_vars <- c("male", "age", "education", "currentSmoker", "cigsPerDay",
"BPMeds", "prevalentStroke", "diabetes", "totChol", "sysBP",
"diaBP", "BMI", "heartRate", "glucose", "TenYearCHD")
# Subset the data
predictors <- framingham_scaled[, predictor_vars]
# Fit logistic regression model
hyp_model <- glm(framingham_scaled$prevalentHyp ~ ., data = predictors, family = "binomial")
# Predict probabilities and labels
hyp_prob <- predict(hyp_model, type = "response")
hyp_pred <- ifelse(hyp_prob > 0.5, 1, 0)
# Actual vs predicted
actual_hyp <- framingham_scaled$prevalentHyp
# Confusion matrix
conf_mat_hyp <- table(Predicted = hyp_pred, Actual = actual_hyp)
TP <- conf_mat_hyp["1", "1"]
TN <- conf_mat_hyp["0", "0"]
FP <- conf_mat_hyp["1", "0"]
FN <- conf_mat_hyp["0", "1"]
# Metrics
accuracy_hyp  <- (TP + TN) / (TP + TN + FP + FN)
precision_hyp <- TP / (TP + FP)
recall_hyp    <- TP / (TP + FN)
f1_score_hyp  <- 2 * precision_hyp * recall_hyp / (precision_hyp + recall_hyp)
# Output results
cat("Classification Metrics for Predicting prevalentHypertension (All Features):\n")
cat("----------------------------------------------------------------------------\n")
cat(sprintf("Accuracy : %.4f\n", accuracy_hyp))
cat(sprintf("Precision: %.4f\n", precision_hyp))
cat(sprintf("Recall   : %.4f\n", recall_hyp))
cat(sprintf("F1 Score : %.4f\n", f1_score_hyp))
cat("\nConfusion Matrix:\n")
print(conf_mat_hyp)
# -----------------------------
# Feature Importance (absolute standardized coefficients)
# -----------------------------
coefs <- summary(hyp_model)$coefficients
feature_importance <- abs(coefs[-1, "Estimate"])  # exclude intercept
importance_df <- data.frame(Feature = names(feature_importance),
Importance = feature_importance)
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]
cat("\nFeature Importance (by standardized coefficient magnitude):\n")
print(importance_df)
# Optional: plot
library(ggplot2)
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "darkred") +
coord_flip() +
labs(title = "Feature Importance for Predicting Hypertension",
x = "Feature", y = "Coefficient Magnitude (Standardized)") +
theme_minimal()
# Load necessary package
library(ggplot2)
# Define all original predictor variables
predictor_vars <- c("male", "age", "education", "currentSmoker", "cigsPerDay",
"BPMeds", "prevalentStroke", "prevalentHyp", "diabetes",
"totChol", "sysBP", "diaBP", "BMI", "heartRate", "glucose")
# Subset predictors and fit the model
predictors <- framingham_scaled[, predictor_vars]
chd_model <- glm(framingham_scaled$TenYearCHD ~ ., data = predictors, family = "binomial")
# Predict probabilities and binary labels
chd_prob <- predict(chd_model, type = "response")
chd_pred <- ifelse(chd_prob > 0.5, 1, 0)
# Actual vs predicted
actual <- framingham_scaled$TenYearCHD
predicted <- chd_pred
# Confusion matrix
conf_mat <- table(Predicted = predicted, Actual = actual)
TP <- conf_mat["1", "1"]
TN <- conf_mat["0", "0"]
FP <- conf_mat["1", "0"]
FN <- conf_mat["0", "1"]
# Classification metrics
accuracy  <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1_score  <- 2 * precision * recall / (precision + recall)
# Print metrics
cat("Classification Metrics for Predicting TenYearCHD (All Features):\n")
cat("-----------------------------------------------------------------\n")
cat(sprintf("Accuracy : %.4f\n", accuracy))
cat(sprintf("Precision: %.4f\n", precision))
cat(sprintf("Recall   : %.4f\n", recall))
cat(sprintf("F1 Score : %.4f\n", f1_score))
cat("\nConfusion Matrix:\n")
print(conf_mat)
# -----------------------------
# Feature Importance (standardized coefficients)
# -----------------------------
coefs <- summary(chd_model)$coefficients
feature_importance <- abs(coefs[-1, "Estimate"])  # remove intercept
importance_df <- data.frame(Feature = names(feature_importance),
Importance = feature_importance)
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]
cat("\nFeature Importance (by standardized coefficient magnitude):\n")
print(importance_df)
# Plot importance
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +
labs(title = "Feature Importance for Predicting TenYearCHD",
x = "Feature", y = "Coefficient Magnitude (Standardized)") +
theme_minimal()
# Install if needed
# install.packages("pROC")
library(pROC)
# Compute ROC curve using actual labels and predicted probabilities
roc_obj <- roc(framingham_scaled$TenYearCHD, framingham_scaled$chd_prob)
# Plot the ROC curve
plot(roc_obj, col = "darkblue", lwd = 2, main = "ROC Curve for TenYearCHD Logistic Model")
abline(a = 0, b = 1, lty = 2, col = "gray")
# Add AUC to the plot
auc_value <- auc(roc_obj)
legend("bottomright", legend = paste("AUC =", round(auc_value, 4)), col = "darkblue", lwd = 2)
# Evaluate recall at multiple thresholds
thresholds <- seq(0.1, 0.5, by = 0.05)
results <- data.frame(Threshold = thresholds, Precision = NA, Recall = NA, F1 = NA)
for (i in seq_along(thresholds)) {
t <- thresholds[i]
pred <- ifelse(framingham_scaled$chd_prob > t, 1, 0)
cm <- table(Predicted = pred, Actual = framingham_scaled$TenYearCHD)
TP <- cm["1", "1"]
TN <- cm["0", "0"]
FP <- cm["1", "0"]
FN <- cm["0", "1"]
prec <- TP / (TP + FP)
rec  <- TP / (TP + FN)
f1   <- 2 * prec * rec / (prec + rec)
results$Precision[i] <- prec
results$Recall[i] <- rec
results$F1[i] <- f1
}
print(results)
# Load required packages
library(ggplot2)
# Define all predictor variables
predictor_vars <- c("male", "age", "education", "currentSmoker", "cigsPerDay",
"BPMeds", "prevalentStroke", "prevalentHyp", "diabetes",
"totChol", "sysBP", "diaBP", "BMI", "heartRate", "glucose")
# Subset the predictors
predictors <- framingham_scaled[, predictor_vars]
# Train logistic regression model with all features
chd_model <- glm(framingham_scaled$TenYearCHD ~ ., data = predictors, family = "binomial")
# Predict probabilities
chd_prob <- predict(chd_model, type = "response")
# Apply threshold optimized for recall
threshold <- 0.15
chd_pred <- ifelse(chd_prob > threshold, 1, 0)
# Actual and predicted values
actual <- framingham_scaled$TenYearCHD
predicted <- chd_pred
# Confusion matrix
conf_mat <- table(Predicted = predicted, Actual = actual)
TP <- conf_mat["1", "1"]
TN <- conf_mat["0", "0"]
FP <- conf_mat["1", "0"]
FN <- conf_mat["0", "1"]
# Evaluation metrics
accuracy  <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1_score  <- 2 * precision * recall / (precision + recall)
# Output results
cat("Logistic Regression for TenYearCHD (Threshold = 0.15)\n")
cat("------------------------------------------------------\n")
cat(sprintf("Accuracy : %.4f\n", accuracy))
cat(sprintf("Precision: %.4f\n", precision))
cat(sprintf("Recall   : %.4f\n", recall))
cat(sprintf("F1 Score : %.4f\n", f1_score))
cat("\nConfusion Matrix:\n")
print(conf_mat)
# Feature Importance (standardized coefficient magnitude)
coefs <- summary(chd_model)$coefficients
feature_importance <- abs(coefs[-1, "Estimate"])  # exclude intercept
importance_df <- data.frame(Feature = names(feature_importance),
Importance = feature_importance)
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]
# Plot
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +
labs(title = "Feature Importance for Predicting TenYearCHD",
x = "Feature", y = "Coefficient Magnitude (Standardized)") +
theme_minimal()
# Load required packages
library(ggplot2)
# Define all predictor variables
predictor_vars <- c("male", "age", "education", "currentSmoker", "cigsPerDay",
"BPMeds", "prevalentStroke", "prevalentHyp", "diabetes",
"totChol", "sysBP", "diaBP", "BMI", "heartRate", "glucose")
# Subset the predictors
predictors <- framingham_scaled[, predictor_vars]
# Train logistic regression model with all features
chd_model <- glm(framingham_scaled$TenYearCHD ~ ., data = predictors, family = "binomial")
# Predict probabilities
chd_prob <- predict(chd_model, type = "response")
# Apply threshold optimized for recall
threshold <- 0.15
chd_pred <- ifelse(chd_prob > threshold, 1, 0)
# Actual and predicted values
actual <- framingham_scaled$TenYearCHD
predicted <- chd_pred
# Confusion matrix
conf_mat <- table(Predicted = predicted, Actual = actual)
TP <- conf_mat["1", "1"]
TN <- conf_mat["0", "0"]
FP <- conf_mat["1", "0"]
FN <- conf_mat["0", "1"]
# Evaluation metrics
accuracy  <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1_score  <- 2 * precision * recall / (precision + recall)
# Output results
cat("Logistic Regression for TenYearCHD (Threshold = 0.15)\n")
cat("------------------------------------------------------\n")
cat(sprintf("Accuracy : %.4f\n", accuracy))
cat(sprintf("Precision: %.4f\n", precision))
cat(sprintf("Recall   : %.4f\n", recall))
cat(sprintf("F1 Score : %.4f\n", f1_score))
cat("\nConfusion Matrix:\n")
print(conf_mat)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Load required libraries
suppressPackageStartupMessages({
library(devtools)
library(dplyr)
library(ggplot2)
library(httr2)
library(readr)
library(tibble)
})
# Load required libraries
#suppressPackageStartupMessages({
# library(devtools)
#library(dplyr)
#library(ggplot2)
#library(httr2)
#library(readr)
#library(tibble)
#})
# Load the PollenDistributionSwiss package
# devtools::load_all(".", quiet = TRUE)
install.packages("./PollenDistributionSwiss_0.1.0.tar.gz", repos = NULL, type = "source")
# Load required libraries
#suppressPackageStartupMessages({
# library(devtools)
#library(dplyr)
#library(ggplot2)
#library(httr2)
#library(readr)
#library(tibble)
#})
# Load the PollenDistributionSwiss package
# devtools::load_all(".", quiet = TRUE)
# sf and terra needs to be insatlled a priori since the installation differs from win/mac/linux
install.packages("./PollenDistributionSwiss_0.1.0.tar.gz", repos = NULL, type = "source")
# Load required libraries
#suppressPackageStartupMessages({
# library(devtools)
#library(dplyr)
#library(ggplot2)
#library(httr2)
#library(readr)
#library(tibble)
#})
# Load the PollenDistributionSwiss package
# devtools::load_all(".", quiet = TRUE)
# sf and terra needs to be insatlled a priori since the installation differs from win/mac/linux
install.packages("./PollenDistributionSwiss_0.1.0.tar.gz", repos = NULL, type = "source")
# Load the PollenDistributionSwiss package
# devtools::load_all(".", quiet = TRUE)
# sf and terra needs to be insatlled a priori since the installation differs from win/mac/linux
install.packages('sf)
# Load the PollenDistributionSwiss package
# devtools::load_all(".", quiet = TRUE)
# sf and terra needs to be insatlled a priori since the installation differs from win/mac/linux
install.packages('sf')
# Load the PollenDistributionSwiss package
# devtools::load_all(".", quiet = TRUE)
# sf and terra needs to be insatlled a priori since the installation differs from win/mac/linux
install.packages('sf')
# Load the PollenDistributionSwiss package
# devtools::load_all(".", quiet = TRUE)
# sf and terra needs to be insatlled a priori since the installation differs from win/mac/linux
install.packages('s2')
# Load the PollenDistributionSwiss package
# devtools::load_all(".", quiet = TRUE)
# sf and terra needs to be insatlled a priori since the installation differs from win/mac/linux
install.packages('sf')
install.packages("s2", type = "source")
install.packages("./PollenDistributionSwiss_0.1.0.tar.gz", repos = NULL, type = "source")
install.packages("PollenDistributionSwiss_0.1.0.tar.gz", repos = NULL, type = "source")
setwd("~/raspnas/universitá/second_year_SP/hackathon/data-hackaton-25-pollen-concentration")
knitr::opts_chunk$set(echo = TRUE)
library(PollenDistributionSwiss)
# Get pollen data for multiple Swiss cities
cities <- c("Geneva", "Lugano")
pollen_data <- get_pollen_for_list_cities(cities, "SwissCities.csv")
# Look at the structure of the data
print("Pollen data structure:")
str(pollen_data)
print("Pollen data:")
print(pollen_data)
# Get detailed forecast for Geneva (coordinates)
geneva_forecast <- get_pollen_forecast(46.2044, 6.1432)
print("Geneva detailed forecast:")
print(geneva_forecast)
# Plot the most common pollen types by canton using our data
plot_most_canton(pollen_data)
# Compare pollen levels between Geneva and Lugano using coordinates
geneva_lat <- 46.2044
geneva_lon <- 6.1432
lugano_lat <- 46.0037
lugano_lon <- 8.9511
comparison_result <- compare_pollen_levels("Geneva", "Lugano",
geneva_lat, geneva_lon,
lugano_lat, lugano_lon)
print(comparison_result)
# Plot specific pollen data using our forecast data
# Note: This function may need the forecast data as input
print("Available pollen types in our data:")
if("plant" %in% names(geneva_forecast)) {
unique_plants <- unique(geneva_forecast$plant[!is.na(geneva_forecast$plant)])
print(unique_plants)
# Plot for the first available pollen type
if(length(unique_plants) > 0) {
plot_specific_pollen(unique_plants[1])
}
}
# Compare pollen levels between Geneva and Lugano using coordinates
geneva_lat <- 46.2044
geneva_lon <- 6.1432
lugano_lat <- 46.0037
lugano_lon <- 8.9511
comparison_result <- compare_pollen_levels("Geneva", "Lugano",
geneva_lat, geneva_lon,
lugano_lat, lugano_lon)
print(comparison_result)
# Summary of pollen levels by city
print("Pollen levels summary:")
if(nrow(pollen_data) > 0) {
print(paste("Cities analyzed:", paste(pollen_data$city, collapse = ", ")))
print(paste("Average grass pollen:", round(mean(pollen_data$grass_pollin, na.rm = TRUE), 2)))
print(paste("Average tree pollen:", round(mean(pollen_data$tree_pollin, na.rm = TRUE), 2)))
# Find city with highest grass pollen
max_grass_city <- pollen_data$city[which.max(pollen_data$grass_pollin)]
print(paste("City with highest grass pollen:", max_grass_city))
}
